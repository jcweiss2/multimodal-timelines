{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b89797-b0e8-463c-8663-a1350a5adea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.util import compile_infix_regex\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3af8fee-ff3f-4fcd-aeaf-ae7a10edf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the following variables first.\n",
    "mimicdir = \"<path to your mimic 3 v1.4 dir>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647901d-dda0-4a11-950b-d714b3c2f12c",
   "metadata": {},
   "source": [
    "## 1. Structured data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c370c-4508-4195-bf79-f6e29269d2c6",
   "metadata": {},
   "source": [
    "This section creates the table for our dataset (`timeline_i2b2_5col_pakdd2024.csv`).\n",
    "- First, run the `mimic3buildtimeline_pakdd.R` R script to create the initial table (`pakdd2024timeline.csv.gz`). You have to set the MIMIC-III directory and the current directory in the script before running it.\n",
    "- Second, run the below cells to check the output file and extract the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfca936-b867-4329-9bb4-efdf0758323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the initial table (the output file of mimic3buildtimeline_pakdd.R) by the MD5 hash\n",
    "r_output_fpath = 'r/extractions/pakdd2024/pakdd2024timeline.csv.gz'\n",
    "\n",
    "def compute_md5(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Read and update hash string value in blocks of 4K\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "assert compute_md5(r_output_fpath) == '965bc0d3568776da0e93072b7463c499'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f1bd81-e53d-47db-b4b3-79de83816dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial table\n",
    "df_struct = pd.read_csv(r_output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bd3b3e-05d8-4b5b-8aaa-3a8ff3c9ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indices in the initial table to extract the structred data our code uses (1031 in total).\n",
    "struct_idx = [28, 29, 63, 62, 64, 145, 146, 147, 260, 153, 152, 266, 1315, 1229, 1228, 3538, 3453, 3452, 3682, 3819, 3909, 3910, 3911, 4000, 4001, 3999, 4051, 4050, 4052, 4054, 4055, 4056, 4057, 4060, 4061,\n",
    "              4062, 4063, 4069, 4073, 4124, 4123, 4125, 4118, 4120, 4121, 4126, 4129, 24487, 24488, 24489, 24490, 24491, 24492, 24493, 24494, 24495, 24510, 24511, 24497, 24498, 24499, 24500, 24501, 24502,\n",
    "              24503, 24504, 24505, 24506, 24507, 24508, 24534, 24535, 24536, 24537, 24538, 24539, 24540, 24599, 24615, 24628, 25172, 25306, 25328, 25872, 25873, 25898, 25913, 25914, 25915, 25929, 25941, 25944,\n",
    "              25942, 25960, 25959, 25961, 25956, 25957, 13055, 13056, 13057, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13069, 13070, 13072, 13073, 13074, 13080, 13081, 13083, 13086, 13087,\n",
    "              13088, 13094, 13097, 13098, 13099, 13100, 13102, 13121, 13128, 13129, 13132, 13133, 13134, 13135, 13141, 13144, 13145, 13219, 13194, 13249, 13238, 13257, 13282, 13264, 13319, 13321, 13322, 13323,\n",
    "              13326, 13327, 13330, 13369, 13356, 13371, 13498, 13524, 13647, 13650, 13651, 13648, 13642, 13645, 13652, 13643, 13653, 13649, 13872, 13910, 13936, 14008, 14080, 14081, 14231, 14232, 14261, 14330,\n",
    "              14329, 14331, 14364, 14367, 14371, 14359, 14361, 14362, 14365, 14372, 14360, 14374, 14366, 14376, 14373, 14368, 14389, 14390, 14419, 14429, 14442, 14447, 14448, 14451, 14456, 14460, 14465, 14490,\n",
    "              14491, 14492, 14493, 14495, 14496, 14497, 14498, 14502, 14503, 14505, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14481, 14482, 14483, 14484, 14485, 14486, 14487,\n",
    "              14488, 14511, 14510, 14512, 14506, 14509, 14507, 14934, 14935, 107730, 107731, 107732, 107733, 107734, 107735, 107736, 107737, 107738, 107739, 107741, 107742, 107743, 107744, 107745, 107746,\n",
    "              107748, 107749, 107750, 107751, 107752, 107754, 107755, 107756, 107757, 107758, 107759, 107760, 107761, 107762, 107763, 107764, 107765, 107766, 107767, 107768, 107769, 107770, 107771, 107772,\n",
    "              107773, 107774, 107775, 107776, 107777, 107778, 107779, 107783, 107784, 107785, 107786, 107787, 107789, 107790, 107798, 107799, 107805, 107812, 107915, 107916, 107930, 108023, 108022, 108133,\n",
    "              108137, 108135, 108136, 108199, 108308, 108365, 108388, 108549, 108550, 108622, 108618, 108617, 109000, 109071, 109093, 109192, 109371, 109389, 109390, 109395, 109392, 109414, 109382, 109383,\n",
    "              109391, 109444, 109446, 109462, 109493, 109509, 109519, 109520, 109558, 109557, 109559, 109556, 109550, 109555, 109549, 109546, 109545, 109553, 109551, 109548, 15171, 15181, 15243, 15242, 15244,\n",
    "              15281, 15395, 15357, 15358, 15359, 15362, 15363, 15424, 15503, 15506, 15531, 15541, 15557, 15567, 15641, 15717, 16037, 16259, 16260, 16238, 16254, 16257, 16388, 17137, 17318, 17720, 17813, 17814,\n",
    "              18527, 18531, 18550, 18960, 18963, 18965, 19222, 19574, 19568, 19571, 19661, 19705, 19677, 19680, 19683, 19809, 19808, 19810, 19814, 19818, 19812, 19815, 19813, 19819, 19821, 19823, 19824, 19825,\n",
    "              19828, 19829, 19830, 19831, 19832, 19833, 19837, 19838, 19839, 19842, 19849, 19846, 19850, 19859, 19858, 19860, 19852, 19854, 19856, 19855, 35451, 35472, 35473, 35510, 35511, 35541, 35546, 35635,\n",
    "              35580, 36735, 37848, 37994, 38015, 38000, 38065, 38120, 38151, 38163, 38169, 38179, 38180, 38207, 38285, 39935, 39940, 40013, 40031, 40030, 40032, 40044, 40043, 40045, 40038, 40041, 40042, 25973,\n",
    "              25980, 26023, 26029, 26037, 26056, 26110, 26242, 26330, 26425, 26420, 26427, 26410, 26415, 26416, 26432, 26418, 26417, 26426, 26428, 26411, 26500, 26818, 27032, 27231, 27373, 27384, 27426, 27449,\n",
    "              27504, 27610, 27703, 28292, 28548, 28547, 28565, 28577, 28576, 28578, 28570, 28567, 28568, 28569, 28574, 8599, 8602, 8632, 8606, 8607, 8616, 8613, 8618, 8619, 8622, 8624, 8631, 8641, 8643, 8647,\n",
    "              8651, 8640, 8610, 8617, 8609, 8615, 8673, 8674, 8688, 8689, 8710, 8713, 8725, 8733, 8739, 9129, 9215, 9281, 9280, 9360, 9388, 9403, 9412, 9552, 9645, 9683, 9843, 9866, 10262, 10523, 10608, 10675,\n",
    "              10808, 11077, 11081, 11083, 11301, 11349, 11415, 11473, 11532, 11637, 11753, 11787, 11852, 11854, 11963, 12015, 12180, 12208, 12209, 12294, 12416, 12429, 12430, 12428, 12466, 12555, 12717, 12854,\n",
    "              12881, 12892, 12893, 12919, 12926, 12936, 12932, 12927, 12957, 12956, 12958, 12959, 21487, 21497, 21498, 21505, 21511, 21624, 21867, 22148, 22099, 22100, 22101, 22173, 22444, 22439, 22510, 22730,\n",
    "              22733, 24089, 24091, 24173, 24199, 24204, 24208, 24224, 24302, 24286, 24348, 24350, 24349, 24406, 24408, 24398, 24404, 28589, 28602, 28603, 28605, 28600, 28604, 28621, 28634, 28635, 28633, 28680,\n",
    "              28788, 28786, 28789, 28939, 28962, 28987, 29035, 30624, 30622, 30620, 30604, 30609, 30621, 30597, 30598, 30605, 30619, 30623, 30790, 30825, 32778, 32779, 32782, 33705, 33877, 33951, 34106, 34112,\n",
    "              34115, 34290, 34405, 34406, 34997, 35124, 35094, 35205, 35239, 35242, 35259, 35276, 35284, 35283, 35309, 35288, 35306, 35307, 35308, 35289, 35290, 35314, 35291, 35300, 35312, 35305, 35292, 35299,\n",
    "              35310, 35313, 35315, 35346, 35338, 35374, 35388, 35389, 35391, 35392, 35393, 35394, 35397, 35398, 35399, 35404, 35409, 35410, 35411, 35412, 35414, 35415, 35417, 35418, 35419, 35422, 35423, 35424,\n",
    "              35439, 35438, 35440, 35437, 35436, 35433, 35434, 105452, 105472, 105473, 105474, 105475, 105476, 105477, 105478, 105479, 105480, 105481, 105482, 105460, 105461, 105462, 105463, 105464, 105465,\n",
    "              105466, 105467, 105468, 105469, 105470, 105471, 105501, 105499, 105533, 105532, 105534, 105681, 106047, 106036, 106038, 106934, 106933, 106935, 106938, 106942, 106941, 106960, 106982, 106981,\n",
    "              106983, 106975, 106974, 106976, 106978, 106980, 106979, 106977, 103781, 103797, 103794, 103795, 103796, 103799, 103800, 103801, 103803, 103804, 103805, 103806, 103808, 103809, 103811, 103814,\n",
    "              103816, 103817, 103818, 103820, 103855, 103858, 103871, 103874, 103870, 103882, 103890, 103886, 103898, 103899, 103895, 103902, 103827, 103832, 103835, 103831, 103849, 103863, 103889, 103848,\n",
    "              103917, 103923, 103931, 103937, 103943, 103948, 103949, 103950, 103947, 103946, 103953, 103957, 103959, 103961, 104025, 104028, 104029, 104078, 104350, 104403, 104407, 104432, 104436, 104598,\n",
    "              104672, 104822, 104841, 104847, 104882, 104883, 104885, 104884, 105356, 105358, 105375, 105379, 105380, 105411, 105438, 105437, 105439, 105433, 105434, 105435, 105436, 19884, 19886, 19887, 19888,\n",
    "              19889, 19893, 19894, 19896, 19897, 19898, 19901, 19903, 19904, 19905, 19877, 19878, 19880, 19881, 19883, 19922, 19926, 19933, 19934, 19948, 19967, 19992, 19970, 20008, 20064, 20061, 20065, 20094,\n",
    "              20091, 20136, 20143, 20142, 20188, 20241, 20242, 20243, 20244, 20332, 20375, 20391, 20384, 20376, 20393, 20386, 20389, 20390, 20378, 20377, 20374, 20381, 20520, 20655, 20683, 20778, 21069, 21149,\n",
    "              21150, 21151, 21152, 21249, 21246, 21293, 21310, 21329, 21350, 21354, 21351, 21357, 21356, 21359, 21367, 21368, 21369, 21376, 21386, 21404, 21387, 21393, 21402, 21407, 21408, 21409, 21411, 21412,\n",
    "              21413, 21414, 21415, 21416, 21417, 21418, 21419, 21420, 21421, 21422, 21423, 21424, 21425, 21426, 21427, 21428, 21429, 21430, 21439, 21438, 21440, 21433, 21437, 21435, 112520, 112521, 112522,\n",
    "              112523, 112524, 112525, 112526, 112527, 112530, 112531, 112533, 112534, 112535, 112538, 112539, 112540, 112541, 112544, 112545, 112546, 112548, 112549, 112550, 112551, 112552, 112553, 112558,\n",
    "              112562, 112563, 112571, 112579, 112619, 112628, 112640, 112649, 112648, 112754, 112755, 112757, 112766, 112777, 112781, 112785, 112789, 112802, 112806, 112818, 112825, 112950, 113145, 113174,\n",
    "              113173, 113175]\n",
    "struct_hids = [112982] * 47 + [133706] * 51 + [145181] * 143 + [145785] * 108 + [152818] * 81 + [158331] * 35 + [160019] * 44 + [162755] * 87 + [163261] * 33 + [164788] * 102 + [178300] * 50 + [184834] * 85 + \\\n",
    "              [125310] * 112 + [157739] * 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b40e58c-6755-4951-a594-49a9f029f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows from the R script output\n",
    "df_struct_new = df_struct.iloc[struct_idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e278671c-186e-47ba-acfe-95cae8545e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from HADM_ID to SUBJECT_ID\n",
    "df_admissions = pd.read_csv(os.path.join(mimicdir, 'ADMISSIONS.csv'))\n",
    "hid_to_subject = {\n",
    "    row.HADM_ID: row.SUBJECT_ID for _, row in df_admissions.iterrows()\n",
    "}\n",
    "\n",
    "# Validate the output file one more time by HADM_ID\n",
    "assert list(map(hid_to_subject.get, struct_hids)) == df_struct_new.pt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc48e71f-763e-4822-8672-23caa2ab367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add admission id column\n",
    "df_struct_new['hid'] = struct_hids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b88b568-5987-4580-8595-c5c1a9e71062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the subject id column and rearrange\n",
    "df_struct_final = df_struct_new[['hid', 't', 'event', 'value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a5ce3b-22cd-400d-a673-d2dc5fa9a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the final table\n",
    "df_struct_final.to_csv('data/timeline_i2b2_5col_pakdd2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e4943-4a3e-44c3-bb22-57f04d2c6171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "617494d8-76a9-4c74-9001-6b859f1548d6",
   "metadata": {},
   "source": [
    "## 2. Generate annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906ca9c-20a1-4ea3-be97-3cd62cc2ac11",
   "metadata": {},
   "source": [
    "This section creates the dataset files under `data/event_pakdd2024_cv/` using the deidentified annotations in `data/annotations_pakdd2024/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca45053-2995-4142-90a8-8e32fd2a6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dir = 'data/annotations_pakdd2024/'\n",
    "dataset_dir = 'data/event_pakdd2024_cv/'\n",
    "hids_all = [112982, 125310, 133706, 145181, 145785, 152818, 157739, 158331, 160019, 162755, 163261, 164788, 178300, 184834]\n",
    "cv_splits = [[163261, 125310, 162755], [160019, 157739, 184834], [158331, 145181, 164788], [145785, 152818, 112982], [133706, 178300]]\n",
    "max_tokens = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67ba5960-7722-4861-85c6-a24a54edce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HADM_ID to admission time\n",
    "hid_to_admit_time = {\n",
    "    row.HADM_ID: row.ADMITTIME for _, row in df_admissions.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13367563-3f86-4537-8e0c-bc184a4f64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from HADM_ID to SUBJECT_ID\n",
    "df_notes = pd.read_csv(os.path.join(mimicdir, 'NOTEEVENTS.csv'), low_memory=False)\n",
    "df_disch = df_notes[df_notes.CATEGORY == 'Discharge summary']\n",
    "hid_to_note = {\n",
    "    row.HADM_ID: row.TEXT.replace('\"', '\"\"') for _, row in df_disch.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcf3ef56-dfb8-4cec-af9a-4e358e144b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "infixes = ['-[**'] + list(nlp.Defaults.infixes)+ ['\\('] + ['\\.\\n'] + ['-\\d+'] + ['-<'] + [':'] + ['].']\n",
    "infix_regex = spacy.util.compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_regex.finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b67b3af3-236f-4732-bc34-9cfead99550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match spacy span using char bounds\n",
    "def get_event_doc_span(st, ed, doc, e):\n",
    "    if doc.text[st:ed] != \"\\n\":\n",
    "        span = doc.char_span(st, ed)\n",
    "        if span is not None:\n",
    "            doc_span = [span.start, span.end]\n",
    "            assert doc[doc_span[0]:doc_span[1]].text== doc.text[st:ed]\n",
    "        else:\n",
    "            # print(\"None Error:\", doc.text[st:ed], st, ed)\n",
    "            if span is None:\n",
    "                new_st, new_ed = st, ed - 1\n",
    "                # print(\"ed - 1\")\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed)\n",
    "\n",
    "            if span is None:\n",
    "                new_st, new_ed = st, ed + 1\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed)\n",
    "                # print(\"ed + 1\")\n",
    "\n",
    "            if span is None:\n",
    "                new_st, new_ed = st + 1, ed\n",
    "                # print(\"st + 1\")\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed)\n",
    "\n",
    "            if span is None:\n",
    "                new_st = st + 1\n",
    "                new_ed = ed - 1 \n",
    "                # print(\"st + 1, ed -1\")\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed) \n",
    "            \n",
    "            if span is None:\n",
    "                new_st = st - 1\n",
    "                new_ed = ed + 1\n",
    "                # print(\"st - 1, ed +1\")\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed)  \n",
    "                \n",
    "            if span is None:\n",
    "                new_st = st - 1\n",
    "                new_ed = ed - 1 \n",
    "                # print(\"st - 1, ed - 1\")\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed) \n",
    "                \n",
    "            if span is None:\n",
    "                new_st = st - 2\n",
    "                new_ed = ed    \n",
    "                # print(\"st - 2, ed \")\n",
    "                if new_st < new_ed:\n",
    "                    span = doc.char_span(new_st, new_ed)\n",
    "                # 1)Bilateral LE doppler\n",
    "\n",
    "            \n",
    "            # print(f\"st:{st}, ed:{ed}\")\n",
    "            # print(\"event\", e)\n",
    "            if span is None:\n",
    "                return None\n",
    "            doc_span = [span.start, span.end]\n",
    "\n",
    "            try:\n",
    "                assert doc[doc_span[0]:doc_span[1]].text== doc.text[new_st:new_ed]\n",
    "            except AssertionError as msg:\n",
    "                # print(msg)\n",
    "                \n",
    "                # print(\"doc[doc_span[0]:doc_span[1]].text\", doc[doc_span[0]:doc_span[1]].text)     \n",
    "                # print(\"doc.text[st:ed]\", doc.text[st:ed]) \n",
    "                # print(f\"{st}:st, {ed}:ed\")\n",
    "                sys.exit()\n",
    "    else:\n",
    "        return None\n",
    "    return doc_span\n",
    "\n",
    "# Extract spacy sentences and annotations aligned to each sentence\n",
    "def get_sentence_entities(one_js, doc):\n",
    "    hadm_id = int(one_js[0]['pt'])\n",
    "    \n",
    "    entities = []\n",
    "    for i, e in enumerate(one_js):\n",
    "        st, ed = e['text_position']['start'], e['text_position']['end']\n",
    "        doc_span = get_event_doc_span(st, ed, doc, e)\n",
    "        if doc_span is not None:\n",
    "            e['doc_span'] = doc_span\n",
    "            e['anno_level'] = 'phrase'\n",
    "            entities.append(e)\n",
    "\n",
    "    entities = sorted(entities, key=lambda x: x['doc_span'][0])\n",
    "\n",
    "    sentence_tokens = []\n",
    "    sent_span = []\n",
    "    entities_out =[]\n",
    "    e_idx = 0\n",
    "    ne = 0\n",
    "    for sent_id, sent in enumerate(doc.sents):\n",
    "        sentence_tokens.append([token.text for token in sent])\n",
    "        sent_span.append([sent.start, sent.end])\n",
    "        entities_out.append([])\n",
    "     \n",
    "        while e_idx < len(entities) and entities[e_idx]['doc_span'][0] < sent.end: \n",
    "            e_v = entities[e_idx]\n",
    "            e_v['doc_span'][-1] = e_v['doc_span'][-1] - 1\n",
    "            entities_out[sent_id].append(e_v['doc_span'] + [e_v['timeline_label_relative']] + [e_v['selected_row_idxs']] + [e_v['anno_level']])\n",
    "            e_idx += 1\n",
    "            ne+=1\n",
    "\n",
    "    assert len(entities) == e_idx == ne\n",
    "    return sentence_tokens,  entities_out\n",
    "\n",
    "def get_abs_time_str(rel_time_sec, admit_time_str):\n",
    "    admit_time = datetime.strptime(admit_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    event_time = admit_time + timedelta(seconds=rel_time_sec)\n",
    "    return datetime.strftime(event_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def get_timeline_label(timeline_label_relative, admit_time_str):\n",
    "    ret = timeline_label_relative.copy()\n",
    "    lb_rel, ub_rel = ret[0], ret[1]\n",
    "    if not isinstance(lb_rel, str) or 'inf' not in lb_rel:\n",
    "        ret[0] = get_abs_time_str(lb_rel, admit_time_str)\n",
    "    if not isinstance(ub_rel, str) or 'inf' not in ub_rel:\n",
    "        ret[1] = get_abs_time_str(ub_rel, admit_time_str)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d15207a-5040-49b2-bd24-735597c97a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3472 examples\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 110\n",
    "examples = []\n",
    "\n",
    "for i, hid in enumerate(hids_all):\n",
    "    anno_fname = f\"annotatinos_{hid}_yes_structured.json\"\n",
    "    with open(os.path.join(anno_dir, anno_fname), 'r') as f:\n",
    "        one_js = json.load(f)\n",
    "    original_text = hid_to_note[hid]\n",
    "    admit_time = hid_to_admit_time[hid]\n",
    "    # print(\"\\nhid\", hid, \"text_len\", len(original_text))\n",
    "    doc = nlp(original_text)\n",
    "\n",
    "    # sentences: list of sentences split by spacy\n",
    "    # entities: examples in each sentences\n",
    "    sentences, entities = get_sentence_entities(one_js, doc)\n",
    "\n",
    "    sentence_start = 0\n",
    "    for sent_idx, pevents in enumerate(entities):\n",
    "        for anno in pevents:\n",
    "            start, end = anno[0], anno[1]\n",
    "            anno_type, timeline_label_relative = anno[2]\n",
    "            selected_row_idxs = anno[3]\n",
    "            anno_level = anno[4]           \n",
    "            \n",
    "            cur_sentence = sentences[sent_idx].copy()            \n",
    "            subj_start = start - sentence_start\n",
    "            subj_end = end - sentence_start\n",
    "            \n",
    "            # First, append next sentence until the end is included in the sentence\n",
    "            sent_idx2 = sent_idx\n",
    "            while subj_end > len(cur_sentence) and len(cur_sentence) < max_tokens:\n",
    "                sent_idx2 += 1\n",
    "                cur_sentence = cur_sentence + sentences[sent_idx2]\n",
    "            \n",
    "            # Increase left context\n",
    "            margin = (max_tokens - len(cur_sentence)) // 2\n",
    "            left = 0\n",
    "            sent_idx_left = sent_idx-1\n",
    "            for sent_idx_left in range(sent_idx-1, -1, -1):\n",
    "                if left + len(sentences[sent_idx_left]) <= margin:\n",
    "                    cur_sentence = sentences[sent_idx_left] + cur_sentence\n",
    "                    left += len(sentences[sent_idx_left])\n",
    "                    subj_start += len(sentences[sent_idx_left])\n",
    "                    subj_end += len(sentences[sent_idx_left])\n",
    "                else:\n",
    "                    sent_idx_left += 1\n",
    "                    break\n",
    "            # Increase right context\n",
    "            margin = max_tokens - len(cur_sentence)\n",
    "            right = 0\n",
    "            for sent_idx_right in range(sent_idx2+1, len(sentences)):\n",
    "                if right + len(sentences[sent_idx_right]) <= margin:\n",
    "                    cur_sentence = cur_sentence + sentences[sent_idx_right]\n",
    "                    right += len(sentences[sent_idx_right])\n",
    "                else:\n",
    "                    break      \n",
    "            # Increase left context again\n",
    "            margin = max_tokens - len(cur_sentence) \n",
    "            for sent_idx_left2 in range(sent_idx_left-1, -1, -1):\n",
    "                if left + len(sentences[sent_idx_left2]) <= margin:\n",
    "                    cur_sentence = sentences[sent_idx_left2] + cur_sentence\n",
    "                    left += len(sentences[sent_idx_left2])\n",
    "                    subj_start += len(sentences[sent_idx_left2])\n",
    "                    subj_end += len(sentences[sent_idx_left2])\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            if subj_end >= len(cur_sentence):\n",
    "                subj_end = len(cur_sentence) - 1\n",
    "            \n",
    "            examples.append({\n",
    "                'docid': hid,\n",
    "                'doc_admit_time': admit_time,\n",
    "                'id': f'{hid}@{admit_time}::({sent_idx})-({start},{end})',\n",
    "                'pevent': get_timeline_label(timeline_label_relative, admit_time),\n",
    "                'subj_start': subj_start,\n",
    "                'subj_end': subj_end,\n",
    "                'subj_label_type': anno_type,\n",
    "                'token': cur_sentence,\n",
    "                'sent_start': 0,\n",
    "                'sent_end': len(cur_sentence),\n",
    "                'selected_row_idxs': selected_row_idxs,\n",
    "                'anno_level': anno_level,\n",
    "            })\n",
    "        sentence_start += len(sentences[sent_idx])\n",
    "\n",
    "print(f\"{len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fae16819-3ebd-474f-9654-fb220cd729d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/event_pakdd2024_cv/0\n",
      "train: 2808\n",
      "dev: 664\n",
      "data/event_pakdd2024_cv/1\n",
      "train: 2819\n",
      "dev: 653\n",
      "data/event_pakdd2024_cv/2\n",
      "train: 2656\n",
      "dev: 816\n",
      "data/event_pakdd2024_cv/3\n",
      "train: 2696\n",
      "dev: 776\n",
      "data/event_pakdd2024_cv/4\n",
      "train: 2909\n",
      "dev: 563\n"
     ]
    }
   ],
   "source": [
    "hids_all2 = [hid for split in cv_splits for hid in split]\n",
    "\n",
    "for cv_idx in range(5):\n",
    "    output_dir = os.path.join(dataset_dir, str(cv_idx))\n",
    "    print(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    dev_hadm_ids = cv_splits[cv_idx]\n",
    "    train_hadm_ids = [h for h in hids_all2 if h not in dev_hadm_ids]\n",
    "\n",
    "    train_examples = [ex for ex in examples if ex['docid'] in train_hadm_ids]\n",
    "    with open(os.path.join(output_dir, 'train.json'), 'w') as fd:\n",
    "        json.dump(train_examples, fd)\n",
    "    print(f'train: {len(train_examples)}')\n",
    "        \n",
    "    dev_examples = [ex for ex in examples if ex['docid'] in dev_hadm_ids]\n",
    "    with open(os.path.join(output_dir, 'dev.json'), 'w') as fd:\n",
    "        json.dump(dev_examples, fd)\n",
    "    print(f'dev: {len(dev_examples)}')\n",
    "\n",
    "    with open(os.path.join(output_dir, 'test.json'), 'w') as fd:\n",
    "        json.dump([], fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6321a0-8dd1-4e0d-a4a0-15a649643cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
